{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42e1a91-1dde-47c9-a97b-e51a670a281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9192/3214510560.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/tmp/ipykernel_9192/3214510560.py:24: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\n",
      "  config_file = pkg_resources.resource_filename('ev2gym', config_file)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, DDPG\n",
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pkg_resources\n",
    "\n",
    "\n",
    "\n",
    "from ev2gym.models.ev2gym_env import EV2Gym\n",
    "from ev2gym.rl_agent.reward import profit_maximization\n",
    "from ev2gym.rl_agent.state import arrival_prices\n",
    "from ev2gym.utilities.callbacks import SaveBestReward\n",
    "from ev2gym.utilities.evaluators import evaluate_model\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "run_name = \"./models/flex_00\"\n",
    "tsb_dir = \"./runs/flex_00\"\n",
    "\n",
    "# we will use an example configuration file\n",
    "config_file = \"/example_config_files/testPST.yaml\"\n",
    "config_file = pkg_resources.resource_filename('ev2gym', config_file)\n",
    "\n",
    "# Creating the environment\n",
    "env = EV2Gym(config_file,\n",
    "             render_mode=False,\n",
    "             save_plots=False,\n",
    "             save_replay=False,\n",
    "             state_function=arrival_prices,\n",
    "             reward_function=profit_maximization,\n",
    "             flex_multiplier=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4baee6-d44c-47b5-aa28-a9692d1f3d84",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b394cc07-7bae-40a9-adcb-f466ffa5ab00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ddpg.ddpg.DDPG at 0x7063ca8855e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "import numpy as np\n",
    "\n",
    "# Create log dir\n",
    "log_dir = run_name+\"/DDPG/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,n_eval_episodes=10,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Add Ornstein-Uhlenbeck noise for exploration\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = DDPG(\"MlpPolicy\",env,learning_rate = 1e-5,action_noise=action_noise,learning_starts=200,tensorboard_log=tsb_dir)     \n",
    "model.learn(total_timesteps=1_000_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb2f336-07e3-42b0-985c-460282aa6df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.9\n",
      "total_profits:  -203.21887722810672\n",
      "real_profits (no flexibility):  -203.21887722810672\n",
      "Up_flexibility (kWh):  1875.4348911719205\n",
      "Down_flexibility (kWh):  884.7603520094126\n",
      "total_energy_charged:  959.8309791925132\n",
      "average_user_satisfaction:  0.9311955695849804\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  -292.83402923\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/DDPG/\"\n",
    "model = DDPG.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c245d-e2b7-4db6-a5db-828bc2b5fc56",
   "metadata": {},
   "source": [
    "## TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ba4c97d-8d50-481a-ab51-5cedffc03bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.td3.td3.TD3 at 0x706396c9be00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import TD3\n",
    "\n",
    "# Create log dir\n",
    "log_dir = run_name+\"/TD3/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,n_eval_episodes=10,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Add Ornstein-Uhlenbeck noise for exploration\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = TD3(\"MlpPolicy\",env,learning_rate = 1e-5,action_noise=action_noise,learning_starts=200,tensorboard_log=tsb_dir)     \n",
    "model.learn(total_timesteps=1_000_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6bd569-a995-43f4-a8c4-1804f28c4cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  45.07\n",
      "total_profits:  -213.25209722683567\n",
      "real_profits (no flexibility):  -213.25209722683567\n",
      "Up_flexibility (kWh):  1737.410875762598\n",
      "Down_flexibility (kWh):  932.805482666069\n",
      "total_energy_charged:  1004.0221735086135\n",
      "average_user_satisfaction:  0.9487971666666666\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  -251.02348299000002\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/TD3/\"\n",
    "model = TD3.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe1bac-c3ea-461a-b6e4-09801c68ee5e",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a19e7289-6fbe-4f71-83f0-4e6aa2add357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x706396c9a4b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = run_name+\"/PPO/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = PPO(\"MlpPolicy\", env,tensorboard_log=tsb_dir)\n",
    "model.learn(total_timesteps=1_500_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e79427bb-5cb2-4e7f-b2e8-8634acddcec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.77\n",
      "total_profits:  -205.55394716056148\n",
      "real_profits (no flexibility):  -205.5539471605615\n",
      "Up_flexibility (kWh):  2406.4221983928355\n",
      "Down_flexibility (kWh):  910.3036013909294\n",
      "total_energy_charged:  981.1211900443396\n",
      "average_user_satisfaction:  0.9418324128351421\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  -253.45348464999998\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/PPO/\"\n",
    "model = PPO.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3285cc-3f8f-4da3-8692-60b6888634a9",
   "metadata": {},
   "source": [
    "## TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "879a587c-e16d-4c56-a9f9-9a1321356b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.trpo.trpo.TRPO at 0x7063ca8fcc50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = run_name+\"/TRPO/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = TRPO(\"MlpPolicy\", env,tensorboard_log=tsb_dir)\n",
    "model.learn(total_timesteps=1_500_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "300d2423-68ff-4df4-8c47-66ce0b6856ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.86\n",
      "total_profits:  -166.92932795353715\n",
      "real_profits (no flexibility):  -166.9293279535371\n",
      "Up_flexibility (kWh):  3016.7304098454456\n",
      "Down_flexibility (kWh):  732.0055808231156\n",
      "total_energy_charged:  803.1130566853323\n",
      "average_user_satisfaction:  0.8610583437047654\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  -445.13449409000003\n"
     ]
    }
   ],
   "source": [
    "from ev2gym.utilities.evaluators import evaluate_model\n",
    "\n",
    "# Load the best model and put the enviroment\n",
    "model = TRPO.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587f60b-110c-4d49-8d8e-224291e7dede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EVthesis",
   "language": "python",
   "name": "evth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
