{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42e1a91-1dde-47c9-a97b-e51a670a281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2320/1602522512.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/tmp/ipykernel_2320/1602522512.py:16: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\n",
      "  config_file = pkg_resources.resource_filename('ev2gym', config_file)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, A2C, DDPG, SAC, TD3\n",
    "import pkg_resources\n",
    "\n",
    "\n",
    "from ev2gym.models.ev2gym_env import EV2Gym\n",
    "# Choose a default reward function and state function or create your own!!!\n",
    "from ev2gym.rl_agent.reward import profit_maximization, SquaredTrackingErrorReward, ProfitMax_TrPenalty_UserIncentives\n",
    "from ev2gym.rl_agent.state import V2G_profit_max, PublicPST, V2G_profit_max_loads\n",
    "from ev2gym.rl_agent.reward import profit_maximization_old\n",
    "from ev2gym.baselines.mpc.V2GProfitMax import V2GProfitMaxOracle\n",
    "from ev2gym.baselines.heuristics import ChargeAsFastAsPossible\n",
    "\n",
    "# we will use an example configuration file\n",
    "config_file = \"/example_config_files/PublicPST.yaml\"\n",
    "config_file = pkg_resources.resource_filename('ev2gym', config_file)\n",
    "\n",
    "# Creating the environment\n",
    "env = EV2Gym(config_file,\n",
    "             render_mode=False,\n",
    "             seed=42,\n",
    "             save_plots=False,\n",
    "             save_replay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b394cc07-7bae-40a9-adcb-f466ffa5ab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -122962.73\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -122962.73 - Last mean reward per episode: -101732.85\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -101732.85 - Last mean reward per episode: -94547.09\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -94547.09 - Last mean reward per episode: nan\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -94547.09 - Last mean reward per episode: -96191.97\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -94547.09 - Last mean reward per episode: -94868.63\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -94547.09 - Last mean reward per episode: -93809.51\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -93809.51 - Last mean reward per episode: -91649.37\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -91649.37 - Last mean reward per episode: -91221.21\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -91221.21 - Last mean reward per episode: -89724.57\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -89724.57 - Last mean reward per episode: -89518.92\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -89518.92 - Last mean reward per episode: -85618.98\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -85618.98 - Last mean reward per episode: -84449.93\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -84449.93 - Last mean reward per episode: -85243.11\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -84449.93 - Last mean reward per episode: -82548.24\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -82548.24 - Last mean reward per episode: -81358.81\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -81358.81 - Last mean reward per episode: -79274.51\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -79274.51 - Last mean reward per episode: -78242.10\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -78242.10 - Last mean reward per episode: -77911.82\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -77911.82 - Last mean reward per episode: -78346.64\n",
      "Num timesteps: 21000\n",
      "Best mean reward: -77911.82 - Last mean reward per episode: -78818.54\n",
      "Num timesteps: 22000\n",
      "Best mean reward: -77911.82 - Last mean reward per episode: -67912.22\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 23000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: -77560.59\n",
      "Num timesteps: 24000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: -77430.37\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: -76380.25\n",
      "Num timesteps: 26000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: nan\n",
      "Num timesteps: 27000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: nan\n",
      "Num timesteps: 28000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: -75647.91\n",
      "Num timesteps: 29000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: nan\n",
      "Num timesteps: 30000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: -76326.68\n",
      "Num timesteps: 31000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: nan\n",
      "Num timesteps: 32000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: -75498.93\n",
      "Num timesteps: 33000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: -74329.10\n",
      "Num timesteps: 34000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: -73234.81\n",
      "Num timesteps: 35000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: -73097.80\n",
      "Num timesteps: 36000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: nan\n",
      "Num timesteps: 37000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: -71449.60\n",
      "Num timesteps: 38000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: -70338.31\n",
      "Num timesteps: 39000\n",
      "Best mean reward: -67912.22 - Last mean reward per episode: -61702.45\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 40000\n",
      "Best mean reward: -61702.45 - Last mean reward per episode: -71157.50\n",
      "Num timesteps: 41000\n",
      "Best mean reward: -61702.45 - Last mean reward per episode: -69375.67\n",
      "Num timesteps: 42000\n",
      "Best mean reward: -61702.45 - Last mean reward per episode: -64844.24\n",
      "Num timesteps: 43000\n",
      "Best mean reward: -61702.45 - Last mean reward per episode: -68943.76\n",
      "Num timesteps: 44000\n",
      "Best mean reward: -61702.45 - Last mean reward per episode: -67274.71\n",
      "Num timesteps: 45000\n",
      "Best mean reward: -61702.45 - Last mean reward per episode: -67730.63\n",
      "Num timesteps: 46000\n",
      "Best mean reward: -61702.45 - Last mean reward per episode: -67544.42\n",
      "Num timesteps: 47000\n",
      "Best mean reward: -61702.45 - Last mean reward per episode: -58537.10\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 48000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -68160.64\n",
      "Num timesteps: 49000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -66731.86\n",
      "Num timesteps: 50000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -66329.27\n",
      "Num timesteps: 51000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -67327.51\n",
      "Num timesteps: 52000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -66079.90\n",
      "Num timesteps: 53000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -65924.76\n",
      "Num timesteps: 54000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -65568.57\n",
      "Num timesteps: 55000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -66145.67\n",
      "Num timesteps: 56000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -64905.78\n",
      "Num timesteps: 57000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -63781.51\n",
      "Num timesteps: 58000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -63862.59\n",
      "Num timesteps: 59000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -62566.84\n",
      "Num timesteps: 60000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -64362.32\n",
      "Num timesteps: 61000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -64085.29\n",
      "Num timesteps: 62000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -63246.76\n",
      "Num timesteps: 63000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -62998.37\n",
      "Num timesteps: 64000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -63554.60\n",
      "Num timesteps: 65000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -62992.00\n",
      "Num timesteps: 66000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -63811.41\n",
      "Num timesteps: 67000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -62965.51\n",
      "Num timesteps: 68000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -62177.08\n",
      "Num timesteps: 69000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -61840.44\n",
      "Num timesteps: 70000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -62095.42\n",
      "Num timesteps: 71000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -62189.43\n",
      "Num timesteps: 72000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -61816.57\n",
      "Num timesteps: 73000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -60923.13\n",
      "Num timesteps: 74000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -61533.65\n",
      "Num timesteps: 75000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -61379.21\n",
      "Num timesteps: 76000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -61218.93\n",
      "Num timesteps: 77000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -60672.81\n",
      "Num timesteps: 78000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -61040.38\n",
      "Num timesteps: 79000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -58622.40\n",
      "Num timesteps: 80000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -59890.29\n",
      "Num timesteps: 81000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -58817.49\n",
      "Num timesteps: 82000\n",
      "Best mean reward: -58537.10 - Last mean reward per episode: -57725.46\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 83000\n",
      "Best mean reward: -57725.46 - Last mean reward per episode: -58717.21\n",
      "Num timesteps: 84000\n",
      "Best mean reward: -57725.46 - Last mean reward per episode: -58346.66\n",
      "Num timesteps: 85000\n",
      "Best mean reward: -57725.46 - Last mean reward per episode: -57527.79\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 86000\n",
      "Best mean reward: -57527.79 - Last mean reward per episode: -58021.23\n",
      "Num timesteps: 87000\n",
      "Best mean reward: -57527.79 - Last mean reward per episode: -57484.05\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 88000\n",
      "Best mean reward: -57484.05 - Last mean reward per episode: -48521.92\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 89000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -51364.16\n",
      "Num timesteps: 90000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -54112.67\n",
      "Num timesteps: 91000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57959.36\n",
      "Num timesteps: 92000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58945.06\n",
      "Num timesteps: 93000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57668.06\n",
      "Num timesteps: 94000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59843.41\n",
      "Num timesteps: 95000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59799.70\n",
      "Num timesteps: 96000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -61045.52\n",
      "Num timesteps: 97000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -60275.86\n",
      "Num timesteps: 98000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59897.29\n",
      "Num timesteps: 99000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59753.62\n",
      "Num timesteps: 100000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -53068.85\n",
      "Num timesteps: 101000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -51739.20\n",
      "Num timesteps: 102000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -61533.80\n",
      "Num timesteps: 103000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -60963.87\n",
      "Num timesteps: 104000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -60545.78\n",
      "Num timesteps: 105000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -60124.26\n",
      "Num timesteps: 106000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -60265.43\n",
      "Num timesteps: 107000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59386.15\n",
      "Num timesteps: 108000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59787.54\n",
      "Num timesteps: 109000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -60634.41\n",
      "Num timesteps: 110000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59358.82\n",
      "Num timesteps: 111000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59126.81\n",
      "Num timesteps: 112000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57272.73\n",
      "Num timesteps: 113000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57208.64\n",
      "Num timesteps: 114000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57958.29\n",
      "Num timesteps: 115000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57735.01\n",
      "Num timesteps: 116000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58571.54\n",
      "Num timesteps: 117000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58916.72\n",
      "Num timesteps: 118000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58874.50\n",
      "Num timesteps: 119000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58590.33\n",
      "Num timesteps: 120000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59299.79\n",
      "Num timesteps: 121000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59576.44\n",
      "Num timesteps: 122000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58507.71\n",
      "Num timesteps: 123000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59668.25\n",
      "Num timesteps: 124000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58823.87\n",
      "Num timesteps: 125000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57934.02\n",
      "Num timesteps: 126000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59513.07\n",
      "Num timesteps: 127000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59028.70\n",
      "Num timesteps: 128000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57506.93\n",
      "Num timesteps: 129000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57777.63\n",
      "Num timesteps: 130000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58446.62\n",
      "Num timesteps: 131000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58269.74\n",
      "Num timesteps: 132000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57744.11\n",
      "Num timesteps: 133000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58051.87\n",
      "Num timesteps: 134000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57314.84\n",
      "Num timesteps: 135000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57230.56\n",
      "Num timesteps: 136000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57266.01\n",
      "Num timesteps: 137000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: nan\n",
      "Num timesteps: 138000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -56928.99\n",
      "Num timesteps: 139000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: nan\n",
      "Num timesteps: 140000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57605.02\n",
      "Num timesteps: 141000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57711.60\n",
      "Num timesteps: 142000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: nan\n",
      "Num timesteps: 143000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57422.45\n",
      "Num timesteps: 144000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59129.82\n",
      "Num timesteps: 145000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: nan\n",
      "Num timesteps: 146000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58890.89\n",
      "Num timesteps: 147000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58693.55\n",
      "Num timesteps: 148000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -59330.23\n",
      "Num timesteps: 149000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58027.53\n",
      "Num timesteps: 150000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58407.77\n",
      "Num timesteps: 151000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57801.22\n",
      "Num timesteps: 152000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57577.09\n",
      "Num timesteps: 153000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57643.28\n",
      "Num timesteps: 154000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -58658.88\n",
      "Num timesteps: 155000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57835.07\n",
      "Num timesteps: 156000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -52558.66\n",
      "Num timesteps: 157000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -57314.87\n",
      "Num timesteps: 158000\n",
      "Best mean reward: -48521.92 - Last mean reward per episode: -47253.67\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 159000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56196.61\n",
      "Num timesteps: 160000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56110.64\n",
      "Num timesteps: 161000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56229.68\n",
      "Num timesteps: 162000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56046.72\n",
      "Num timesteps: 163000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56022.97\n",
      "Num timesteps: 164000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55694.25\n",
      "Num timesteps: 165000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56063.98\n",
      "Num timesteps: 166000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: nan\n",
      "Num timesteps: 167000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: nan\n",
      "Num timesteps: 168000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56585.88\n",
      "Num timesteps: 169000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57588.80\n",
      "Num timesteps: 170000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57678.95\n",
      "Num timesteps: 171000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57584.17\n",
      "Num timesteps: 172000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56690.53\n",
      "Num timesteps: 173000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57784.14\n",
      "Num timesteps: 174000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58109.33\n",
      "Num timesteps: 175000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58281.41\n",
      "Num timesteps: 176000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58118.37\n",
      "Num timesteps: 177000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -54337.37\n",
      "Num timesteps: 178000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57896.43\n",
      "Num timesteps: 179000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58335.87\n",
      "Num timesteps: 180000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56674.33\n",
      "Num timesteps: 181000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57819.52\n",
      "Num timesteps: 182000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57017.25\n",
      "Num timesteps: 183000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57411.45\n",
      "Num timesteps: 184000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57578.32\n",
      "Num timesteps: 185000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57353.40\n",
      "Num timesteps: 186000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55879.90\n",
      "Num timesteps: 187000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56885.10\n",
      "Num timesteps: 188000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58109.07\n",
      "Num timesteps: 189000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56707.29\n",
      "Num timesteps: 190000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56667.84\n",
      "Num timesteps: 191000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57743.17\n",
      "Num timesteps: 192000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57585.97\n",
      "Num timesteps: 193000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57269.70\n",
      "Num timesteps: 194000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58051.08\n",
      "Num timesteps: 195000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57683.34\n",
      "Num timesteps: 196000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57823.50\n",
      "Num timesteps: 197000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -59164.54\n",
      "Num timesteps: 198000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58010.53\n",
      "Num timesteps: 199000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57454.58\n",
      "Num timesteps: 200000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58953.09\n",
      "Num timesteps: 201000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57997.81\n",
      "Num timesteps: 202000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58048.28\n",
      "Num timesteps: 203000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56684.03\n",
      "Num timesteps: 204000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57778.14\n",
      "Num timesteps: 205000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58644.07\n",
      "Num timesteps: 206000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56793.98\n",
      "Num timesteps: 207000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57096.86\n",
      "Num timesteps: 208000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57115.82\n",
      "Num timesteps: 209000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57109.96\n",
      "Num timesteps: 210000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55307.50\n",
      "Num timesteps: 211000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57729.83\n",
      "Num timesteps: 212000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57359.12\n",
      "Num timesteps: 213000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56692.67\n",
      "Num timesteps: 214000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -51103.26\n",
      "Num timesteps: 215000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57754.33\n",
      "Num timesteps: 216000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57070.06\n",
      "Num timesteps: 217000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57084.43\n",
      "Num timesteps: 218000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: nan\n",
      "Num timesteps: 219000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56986.12\n",
      "Num timesteps: 220000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56508.06\n",
      "Num timesteps: 221000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -52016.54\n",
      "Num timesteps: 222000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56850.25\n",
      "Num timesteps: 223000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56278.85\n",
      "Num timesteps: 224000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56332.77\n",
      "Num timesteps: 225000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56915.09\n",
      "Num timesteps: 226000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56673.06\n",
      "Num timesteps: 227000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56188.94\n",
      "Num timesteps: 228000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56709.37\n",
      "Num timesteps: 229000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56897.23\n",
      "Num timesteps: 230000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56743.37\n",
      "Num timesteps: 231000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -50085.00\n",
      "Num timesteps: 232000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -54333.01\n",
      "Num timesteps: 233000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56117.61\n",
      "Num timesteps: 234000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56454.12\n",
      "Num timesteps: 235000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56782.06\n",
      "Num timesteps: 236000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57379.55\n",
      "Num timesteps: 237000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56537.46\n",
      "Num timesteps: 238000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55816.82\n",
      "Num timesteps: 239000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55105.22\n",
      "Num timesteps: 240000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55523.34\n",
      "Num timesteps: 241000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56180.93\n",
      "Num timesteps: 242000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55479.01\n",
      "Num timesteps: 243000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55869.33\n",
      "Num timesteps: 244000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56037.15\n",
      "Num timesteps: 245000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56378.96\n",
      "Num timesteps: 246000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56189.76\n",
      "Num timesteps: 247000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55324.34\n",
      "Num timesteps: 248000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57110.78\n",
      "Num timesteps: 249000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58168.75\n",
      "Num timesteps: 250000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57072.17\n",
      "Num timesteps: 251000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57404.55\n",
      "Num timesteps: 252000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55787.87\n",
      "Num timesteps: 253000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55799.33\n",
      "Num timesteps: 254000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56057.39\n",
      "Num timesteps: 255000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57680.25\n",
      "Num timesteps: 256000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58452.77\n",
      "Num timesteps: 257000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -58168.89\n",
      "Num timesteps: 258000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57627.26\n",
      "Num timesteps: 259000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57432.83\n",
      "Num timesteps: 260000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57028.22\n",
      "Num timesteps: 261000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56285.92\n",
      "Num timesteps: 262000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -50127.67\n",
      "Num timesteps: 263000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55259.22\n",
      "Num timesteps: 264000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -48137.29\n",
      "Num timesteps: 265000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -52148.34\n",
      "Num timesteps: 266000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -57228.98\n",
      "Num timesteps: 267000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55604.94\n",
      "Num timesteps: 268000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56511.42\n",
      "Num timesteps: 269000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56447.19\n",
      "Num timesteps: 270000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -56412.70\n",
      "Num timesteps: 271000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55666.93\n",
      "Num timesteps: 272000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55976.21\n",
      "Num timesteps: 273000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55802.53\n",
      "Num timesteps: 274000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -54934.67\n",
      "Num timesteps: 275000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55213.34\n",
      "Num timesteps: 276000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55561.36\n",
      "Num timesteps: 277000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -55085.18\n",
      "Num timesteps: 278000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -54388.47\n",
      "Num timesteps: 279000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -53648.91\n",
      "Num timesteps: 280000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -52650.84\n",
      "Num timesteps: 281000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -53185.62\n",
      "Num timesteps: 282000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -53842.07\n",
      "Num timesteps: 283000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -53709.32\n",
      "Num timesteps: 284000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -53916.47\n",
      "Num timesteps: 285000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -53569.64\n",
      "Num timesteps: 286000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -52814.32\n",
      "Num timesteps: 287000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -53441.65\n",
      "Num timesteps: 288000\n",
      "Best mean reward: -47253.67 - Last mean reward per episode: -46254.40\n",
      "Saving new best model to /tmp/gym_public_tracking/best_model.zip\n",
      "Num timesteps: 289000\n",
      "Best mean reward: -46254.40 - Last mean reward per episode: -55023.51\n",
      "Num timesteps: 290000\n",
      "Best mean reward: -46254.40 - Last mean reward per episode: -54908.45\n",
      "Num timesteps: 291000\n",
      "Best mean reward: -46254.40 - Last mean reward per episode: -54788.88\n",
      "Num timesteps: 292000\n",
      "Best mean reward: -46254.40 - Last mean reward per episode: -54877.62\n",
      "Num timesteps: 293000\n",
      "Best mean reward: -46254.40 - Last mean reward per episode: -55133.17\n",
      "Num timesteps: 294000\n",
      "Best mean reward: -46254.40 - Last mean reward per episode: -54822.80\n",
      "Num timesteps: 295000\n",
      "Best mean reward: -46254.40 - Last mean reward per episode: -53964.01\n",
      "Num timesteps: 296000\n",
      "Best mean reward: -46254.40 - Last mean reward per episode: -55196.80\n",
      "Num timesteps: 297000\n",
      "Best mean reward: -46254.40 - Last mean reward per episode: -54804.89\n",
      "Num timesteps: 298000\n",
      "Best mean reward: -46254.40 - Last mean reward per episode: -54180.76\n",
      "Num timesteps: 299000\n",
      "Best mean reward: -46254.40 - Last mean reward per episode: -54344.44\n",
      "Num timesteps: 300000\n",
      "Best mean reward: -46254.40 - Last mean reward per episode: -55825.38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ddpg.ddpg.DDPG at 0x7b802c039280>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ev2gym.utilities.callbacks import SaveBestReward\n",
    "\n",
    "\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = DDPG(\"MlpPolicy\", env)\n",
    "model.learn(total_timesteps=300_000,callback=SaveBestReward(check_freq=1000, log_dir=log_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7296688-b009-4e32-9100-2f12d9e19788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  47.55\n",
      "total_profits:  -145.63963894418117\n",
      "total_energy_charged:  682.708381102632\n",
      "total_energy_discharged:  0.0\n",
      "average_user_satisfaction:  0.9658472219067946\n",
      "power_tracker_violation:  536.5151099668903\n",
      "tracking_error:  59305.55405289035\n",
      "energy_user_satisfaction:  100.0\n",
      "total_transformer_overload:  40.28489579592966\n",
      "reward:  -56659.85376855\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "model = DDPG.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "from ev2gym.utilities.evaluators import evaluate_model\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2f336-07e3-42b0-985c-460282aa6df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EVthesis",
   "language": "python",
   "name": "evth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
