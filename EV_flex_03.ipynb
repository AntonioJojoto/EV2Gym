{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42e1a91-1dde-47c9-a97b-e51a670a281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2173/480342740.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/tmp/ipykernel_2173/480342740.py:24: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\n",
      "  config_file = pkg_resources.resource_filename('ev2gym', config_file)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, DDPG\n",
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pkg_resources\n",
    "\n",
    "\n",
    "\n",
    "from ev2gym.models.ev2gym_env import EV2Gym\n",
    "from ev2gym.rl_agent.reward import profit_maximization\n",
    "from ev2gym.rl_agent.state import arrival_prices_flex\n",
    "from ev2gym.utilities.callbacks import SaveBestReward\n",
    "from ev2gym.utilities.evaluators import evaluate_model\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "run_name = \"./models/flex_03\"\n",
    "tsb_dir = \"./runs/flex_03\"\n",
    "\n",
    "# we will use an example configuration file\n",
    "config_file = \"/example_config_files/testPST.yaml\"\n",
    "config_file = pkg_resources.resource_filename('ev2gym', config_file)\n",
    "\n",
    "# Creating the environment\n",
    "env = EV2Gym(config_file,\n",
    "             render_mode=False,\n",
    "             save_plots=False,\n",
    "             save_replay=False,\n",
    "             state_function=arrival_prices_flex,\n",
    "             reward_function=profit_maximization,\n",
    "             flex_multiplier=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4baee6-d44c-47b5-aa28-a9692d1f3d84",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b394cc07-7bae-40a9-adcb-f466ffa5ab00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ddpg.ddpg.DDPG at 0x7eb26d98f950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "import numpy as np\n",
    "\n",
    "# Create log dir\n",
    "log_dir = run_name+\"/DDPG/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,n_eval_episodes=10,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Add Ornstein-Uhlenbeck noise for exploration\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = DDPG(\"MlpPolicy\",env,learning_rate = 1e-5,action_noise=action_noise,learning_starts=200,tensorboard_log=tsb_dir)     \n",
    "model.learn(total_timesteps=1_000_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb2f336-07e3-42b0-985c-460282aa6df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.98\n",
      "total_profits:  -33.489596809091246\n",
      "real_profits (no flexibility):  -218.14663983851833\n",
      "Up_flexibility (kWh):  1963.5700537701966\n",
      "Down_flexibility (kWh):  950.7790969254122\n",
      "total_energy_charged:  1042.4141537210721\n",
      "average_user_satisfaction:  0.9646574772305226\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  -21.63537718\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/DDPG/\"\n",
    "model = DDPG.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40c966-d83e-4363-a517-10301ff05665",
   "metadata": {},
   "source": [
    "## TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f658bf3f-4ad6-4ff1-8ad9-58bfa6008f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.td3.td3.TD3 at 0x7eb240293770>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import TD3\n",
    "\n",
    "# Create log dir\n",
    "log_dir = run_name+\"/TD3/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,n_eval_episodes=10,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Add Ornstein-Uhlenbeck noise for exploration\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = TD3(\"MlpPolicy\",env,learning_rate = 1e-5,action_noise=action_noise,learning_starts=200,tensorboard_log=tsb_dir)     \n",
    "model.learn(total_timesteps=1_000_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85fdc55-4914-4546-871f-b5a0154b1918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.65\n",
      "total_profits:  -23.144245878367748\n",
      "real_profits (no flexibility):  -215.75249369313846\n",
      "Up_flexibility (kWh):  2084.2577477627856\n",
      "Down_flexibility (kWh):  952.4924312823155\n",
      "total_energy_charged:  1040.2395835589666\n",
      "average_user_satisfaction:  0.966474888888889\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  -3.71729601\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/TD3/\"\n",
    "model = TD3.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe1bac-c3ea-461a-b6e4-09801c68ee5e",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19e7289-6fbe-4f71-83f0-4e6aa2add357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7d44c9329640>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = run_name+\"/PPO/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = PPO(\"MlpPolicy\", env,tensorboard_log=tsb_dir)\n",
    "model.learn(total_timesteps=1_500_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e79427bb-5cb2-4e7f-b2e8-8634acddcec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.41\n",
      "total_profits:  52.45044715862293\n",
      "real_profits (no flexibility):  -194.01925881888133\n",
      "Up_flexibility (kWh):  3052.449722735067\n",
      "Down_flexibility (kWh):  866.9115167475363\n",
      "total_energy_charged:  937.0015779382924\n",
      "average_user_satisfaction:  0.9247097253325005\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  1.2815920400000003\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/PPO/\"\n",
    "model = PPO.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3285cc-3f8f-4da3-8692-60b6888634a9",
   "metadata": {},
   "source": [
    "## TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879a587c-e16d-4c56-a9f9-9a1321356b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.trpo.trpo.TRPO at 0x7d449797d6a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = run_name+\"/TRPO/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = TRPO(\"MlpPolicy\", env,tensorboard_log=tsb_dir)\n",
    "model.learn(total_timesteps=1_500_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300d2423-68ff-4df4-8c47-66ce0b6856ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  45.04\n",
      "total_profits:  44.61334263057168\n",
      "real_profits (no flexibility):  -201.4307806601092\n",
      "Up_flexibility (kWh):  3008.4717328160677\n",
      "Down_flexibility (kWh):  903.0442402686803\n",
      "total_energy_charged:  975.7644871200686\n",
      "average_user_satisfaction:  0.9356206210256977\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  17.51032253\n"
     ]
    }
   ],
   "source": [
    "from ev2gym.utilities.evaluators import evaluate_model\n",
    "\n",
    "# Load the best model and put the enviroment\n",
    "model = TRPO.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52920cb5-af4b-4c65-897c-30b4c2897d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EVthesis",
   "language": "python",
   "name": "evth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
