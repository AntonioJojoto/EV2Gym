{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42e1a91-1dde-47c9-a97b-e51a670a281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2249/3726084541.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/tmp/ipykernel_2249/3726084541.py:24: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\n",
      "  config_file = pkg_resources.resource_filename('ev2gym', config_file)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, DDPG\n",
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pkg_resources\n",
    "\n",
    "\n",
    "\n",
    "from ev2gym.models.ev2gym_env import EV2Gym\n",
    "from ev2gym.rl_agent.reward import profit_maximization\n",
    "from ev2gym.rl_agent.state import arrival_prices_flex\n",
    "from ev2gym.utilities.callbacks import SaveBestReward\n",
    "from ev2gym.utilities.evaluators import evaluate_model\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "run_name = \"./models/flex_05\"\n",
    "tsb_dir = \"./runs/flex_05\"\n",
    "\n",
    "# we will use an example configuration file\n",
    "config_file = \"/example_config_files/testPST.yaml\"\n",
    "config_file = pkg_resources.resource_filename('ev2gym', config_file)\n",
    "\n",
    "# Creating the environment\n",
    "env = EV2Gym(config_file,\n",
    "             render_mode=False,\n",
    "             save_plots=False,\n",
    "             save_replay=False,\n",
    "             state_function=arrival_prices_flex,\n",
    "             reward_function=profit_maximization,\n",
    "             flex_multiplier=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4baee6-d44c-47b5-aa28-a9692d1f3d84",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b394cc07-7bae-40a9-adcb-f466ffa5ab00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ddpg.ddpg.DDPG at 0x7dfd1a7cb560>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "import numpy as np\n",
    "\n",
    "# Create log dir\n",
    "log_dir = run_name+\"/DDPG/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,n_eval_episodes=10,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Add Ornstein-Uhlenbeck noise for exploration\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = DDPG(\"MlpPolicy\",env,learning_rate = 1e-5,action_noise=action_noise,learning_starts=200,tensorboard_log=tsb_dir)     \n",
    "model.learn(total_timesteps=1_000_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb2f336-07e3-42b0-985c-460282aa6df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.87\n",
      "total_profits:  147.5350300887514\n",
      "real_profits (no flexibility):  -203.53371897611666\n",
      "Up_flexibility (kWh):  2439.569143889163\n",
      "Down_flexibility (kWh):  894.3344429837479\n",
      "total_energy_charged:  984.1995317676939\n",
      "average_user_satisfaction:  0.9405596788812971\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  96.14054383000001\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/DDPG/\"\n",
    "model = DDPG.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40c966-d83e-4363-a517-10301ff05665",
   "metadata": {},
   "source": [
    "## TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f658bf3f-4ad6-4ff1-8ad9-58bfa6008f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.td3.td3.TD3 at 0x7dfce51bf590>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import TD3\n",
    "\n",
    "# Create log dir\n",
    "log_dir = run_name+\"/TD3/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,n_eval_episodes=10,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Add Ornstein-Uhlenbeck noise for exploration\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = TD3(\"MlpPolicy\",env,learning_rate = 1e-5,action_noise=action_noise,learning_starts=200,tensorboard_log=tsb_dir)     \n",
    "model.learn(total_timesteps=1_000_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85fdc55-4914-4546-871f-b5a0154b1918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.4\n",
      "total_profits:  108.72542975706169\n",
      "real_profits (no flexibility):  -214.7031065156672\n",
      "Up_flexibility (kWh):  2115.265495372967\n",
      "Down_flexibility (kWh):  952.5311170475375\n",
      "total_energy_charged:  1038.6634555483638\n",
      "average_user_satisfaction:  0.9684406078485149\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  128.32976156\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/TD3/\"\n",
    "model = TD3.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe1bac-c3ea-461a-b6e4-09801c68ee5e",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19e7289-6fbe-4f71-83f0-4e6aa2add357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x72f8fc7206b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = run_name+\"/PPO/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = PPO(\"MlpPolicy\", env,tensorboard_log=tsb_dir)\n",
    "model.learn(total_timesteps=1_500_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e79427bb-5cb2-4e7f-b2e8-8634acddcec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.76\n",
      "total_profits:  231.85351238710234\n",
      "real_profits (no flexibility):  -189.50799092606067\n",
      "Up_flexibility (kWh):  3172.2630174070828\n",
      "Down_flexibility (kWh):  845.108280012721\n",
      "total_energy_charged:  916.9209984109807\n",
      "average_user_satisfaction:  0.9112641074779606\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  149.00142169999998\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/PPO/\"\n",
    "model = PPO.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3285cc-3f8f-4da3-8692-60b6888634a9",
   "metadata": {},
   "source": [
    "## TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879a587c-e16d-4c56-a9f9-9a1321356b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.trpo.trpo.TRPO at 0x72f901cc17c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = run_name+\"/TRPO/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = TRPO(\"MlpPolicy\", env,tensorboard_log=tsb_dir)\n",
    "model.learn(total_timesteps=1_500_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300d2423-68ff-4df4-8c47-66ce0b6856ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.95\n",
      "total_profits:  229.01769186926097\n",
      "real_profits (no flexibility):  -188.15350853807257\n",
      "Up_flexibility (kWh):  3137.519440908586\n",
      "Down_flexibility (kWh):  839.472902444527\n",
      "total_energy_charged:  910.0627421210837\n",
      "average_user_satisfaction:  0.9078805500253152\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  129.56615868\n"
     ]
    }
   ],
   "source": [
    "from ev2gym.utilities.evaluators import evaluate_model\n",
    "\n",
    "# Load the best model and put the enviroment\n",
    "model = TRPO.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02a061-abc3-476a-b6cc-97faae87a8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EVthesis",
   "language": "python",
   "name": "evth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
