{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42e1a91-1dde-47c9-a97b-e51a670a281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15020/3946343970.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/tmp/ipykernel_15020/3946343970.py:24: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\n",
      "  config_file = pkg_resources.resource_filename('ev2gym', config_file)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, A2C, DDPG\n",
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pkg_resources\n",
    "\n",
    "\n",
    "\n",
    "from ev2gym.models.ev2gym_env import EV2Gym\n",
    "from ev2gym.rl_agent.reward import profit_maximization_old\n",
    "from ev2gym.rl_agent.state import arrival_prices\n",
    "from ev2gym.utilities.callbacks import SaveBestReward\n",
    "from ev2gym.utilities.evaluators import evaluate_model\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "run_name = \"./models/econ_public_old\"\n",
    "tsb_dir = \"./runs/econ_public_old\"\n",
    "\n",
    "# we will use an example configuration file\n",
    "config_file = \"/example_config_files/PublicPST.yaml\"\n",
    "config_file = pkg_resources.resource_filename('ev2gym', config_file)\n",
    "\n",
    "# Creating the environment\n",
    "env = EV2Gym(config_file,\n",
    "             render_mode=False,\n",
    "             save_plots=False,\n",
    "             save_replay=False,\n",
    "             state_function=arrival_prices,\n",
    "             reward_function=profit_maximization_old,\n",
    "             flex_multiplier=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4baee6-d44c-47b5-aa28-a9692d1f3d84",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b394cc07-7bae-40a9-adcb-f466ffa5ab00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ddpg.ddpg.DDPG at 0x793236866480>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = run_name+\"/DDPG/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,n_eval_episodes=10,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = DDPG(\"MlpPolicy\",env,learning_rate = 1e-5,learning_starts=200,tensorboard_log=tsb_dir)     \n",
    "model.learn(total_timesteps=100_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb2f336-07e3-42b0-985c-460282aa6df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  48.23\n",
      "total_profits:  -101.28582761033849\n",
      "real_profits (no flexibility):  -101.28582761033849\n",
      "Up_flexibility (kWh):  4433.3512253783465\n",
      "Down_flexibility (kWh):  400.24381895586953\n",
      "total_energy_charged:  470.336973898633\n",
      "average_user_satisfaction:  0.8774489905677373\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  -106.24679101\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "model = DDPG.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe1bac-c3ea-461a-b6e4-09801c68ee5e",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a19e7289-6fbe-4f71-83f0-4e6aa2add357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7931cf12bf80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = PPO(\"MlpPolicy\", env,tensorboard_log=tsb_dir)\n",
    "model.learn(total_timesteps=1_000_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e79427bb-5cb2-4e7f-b2e8-8634acddcec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  47.78\n",
      "total_profits:  -14.352098505076547\n",
      "real_profits (no flexibility):  -14.352098505076547\n",
      "Up_flexibility (kWh):  6319.869136975935\n",
      "Down_flexibility (kWh):  54.76303417701002\n",
      "total_energy_charged:  66.19514850404144\n",
      "average_user_satisfaction:  0.7079861892316631\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  -31.09490499\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/PPO/\"\n",
    "model = PPO.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3285cc-3f8f-4da3-8692-60b6888634a9",
   "metadata": {},
   "source": [
    "## TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879a587c-e16d-4c56-a9f9-9a1321356b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.trpo.trpo.TRPO at 0x7931cf1a3e90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = run_name+\"/TRPO/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = TRPO(\"MlpPolicy\", env,tensorboard_log=tsb_dir)\n",
    "model.learn(total_timesteps=500_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300d2423-68ff-4df4-8c47-66ce0b6856ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  47.72\n",
      "total_profits:  -0.7786157839086616\n",
      "real_profits (no flexibility):  -0.7786157839086616\n",
      "Up_flexibility (kWh):  6611.702966931937\n",
      "Down_flexibility (kWh):  1.5777462809163751\n",
      "total_energy_charged:  3.209372956471762\n",
      "average_user_satisfaction:  0.683489194889568\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  -19.71453162\n"
     ]
    }
   ],
   "source": [
    "from ev2gym.utilities.evaluators import evaluate_model\n",
    "\n",
    "# Load the best model and put the enviroment\n",
    "model = TRPO.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd5634-31da-4030-bce1-e980a2b57d4c",
   "metadata": {},
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34196c72-864d-4511-86f5-0afadf0f87a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x793330628c50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = run_name+\"/A2C/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = A2C(\"MlpPolicy\", env,tensorboard_log=tsb_dir)\n",
    "model.learn(total_timesteps=300_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a59ee44a-27e9-4335-8923-28874182e7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  48.04\n",
      "total_profits:  -9.032473151965185\n",
      "real_profits (no flexibility):  -9.032473151965185\n",
      "Up_flexibility (kWh):  6323.872558506583\n",
      "Down_flexibility (kWh):  33.916457773240694\n",
      "total_energy_charged:  40.644331336684026\n",
      "average_user_satisfaction:  0.7013681138651993\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  -27.138468720000002\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "model = A2C.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62148591-e255-435c-9dd4-990e875cee56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EVthesis",
   "language": "python",
   "name": "evth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
