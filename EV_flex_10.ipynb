{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42e1a91-1dde-47c9-a97b-e51a670a281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2330/4183270682.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/tmp/ipykernel_2330/4183270682.py:24: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\n",
      "  config_file = pkg_resources.resource_filename('ev2gym', config_file)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, DDPG\n",
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pkg_resources\n",
    "\n",
    "\n",
    "\n",
    "from ev2gym.models.ev2gym_env import EV2Gym\n",
    "from ev2gym.rl_agent.reward import profit_maximization\n",
    "from ev2gym.rl_agent.state import arrival_prices_flex\n",
    "from ev2gym.utilities.callbacks import SaveBestReward\n",
    "from ev2gym.utilities.evaluators import evaluate_model\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "run_name = \"./models/flex_10\"\n",
    "tsb_dir = \"./runs/flex_10\"\n",
    "\n",
    "# we will use an example configuration file\n",
    "config_file = \"/example_config_files/testPST.yaml\"\n",
    "config_file = pkg_resources.resource_filename('ev2gym', config_file)\n",
    "\n",
    "# Creating the environment\n",
    "env = EV2Gym(config_file,\n",
    "             render_mode=False,\n",
    "             save_plots=False,\n",
    "             save_replay=False,\n",
    "             state_function=arrival_prices_flex,\n",
    "             reward_function=profit_maximization,\n",
    "             flex_multiplier=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4baee6-d44c-47b5-aa28-a9692d1f3d84",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b394cc07-7bae-40a9-adcb-f466ffa5ab00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ddpg.ddpg.DDPG at 0x7741b1501d60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "import numpy as np\n",
    "\n",
    "# Create log dir\n",
    "log_dir = run_name+\"/DDPG/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,n_eval_episodes=10,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Add Ornstein-Uhlenbeck noise for exploration\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = DDPG(\"MlpPolicy\",env,learning_rate = 1e-5,action_noise=action_noise,learning_starts=200,tensorboard_log=tsb_dir)     \n",
    "model.learn(total_timesteps=1_000_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb2f336-07e3-42b0-985c-460282aa6df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  45.17\n",
      "total_profits:  624.5964302126539\n",
      "real_profits (no flexibility):  -156.60124515761302\n",
      "Up_flexibility (kWh):  3014.423455625413\n",
      "Down_flexibility (kWh):  692.9296843283894\n",
      "total_energy_charged:  763.0921378431468\n",
      "average_user_satisfaction:  0.8378252148908448\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  232.60708864\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/DDPG/\"\n",
    "model = DDPG.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40c966-d83e-4363-a517-10301ff05665",
   "metadata": {},
   "source": [
    "## TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f658bf3f-4ad6-4ff1-8ad9-58bfa6008f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.td3.td3.TD3 at 0x754ef69c8380>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "import numpy as np\n",
    "\n",
    "# Create log dir\n",
    "log_dir = run_name+\"/TD3/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,n_eval_episodes=10,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Add Ornstein-Uhlenbeck noise for exploration\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = TD3(\"MlpPolicy\",env,learning_rate = 1e-5,action_noise=action_noise,learning_starts=200,tensorboard_log=tsb_dir)     \n",
    "model.learn(total_timesteps=1_000_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85fdc55-4914-4546-871f-b5a0154b1918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.62\n",
      "total_profits:  511.68327129734536\n",
      "real_profits (no flexibility):  -194.11203544188066\n",
      "Up_flexibility (kWh):  2508.208126230365\n",
      "Down_flexibility (kWh):  844.3233206677866\n",
      "total_energy_charged:  939.2745898194783\n",
      "average_user_satisfaction:  0.9205772240104005\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  409.35906406000004\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/TD3/\"\n",
    "model = TD3.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe1bac-c3ea-461a-b6e4-09801c68ee5e",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a19e7289-6fbe-4f71-83f0-4e6aa2add357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x754ef69fb7d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = run_name+\"/PPO/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = PPO(\"MlpPolicy\", env,tensorboard_log=tsb_dir)\n",
    "model.learn(total_timesteps=1_500_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e79427bb-5cb2-4e7f-b2e8-8634acddcec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.75\n",
      "total_profits:  706.2078487217357\n",
      "real_profits (no flexibility):  -160.92930420053762\n",
      "Up_flexibility (kWh):  3420.5370979960962\n",
      "Down_flexibility (kWh):  720.3878498506108\n",
      "total_energy_charged:  776.4125694569368\n",
      "average_user_satisfaction:  0.8493201508764616\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  447.99686321\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and put the enviroment\n",
    "log_dir = run_name+\"/PPO/\"\n",
    "model = PPO.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3285cc-3f8f-4da3-8692-60b6888634a9",
   "metadata": {},
   "source": [
    "## TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "879a587c-e16d-4c56-a9f9-9a1321356b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.trpo.trpo.TRPO at 0x7444c5759ca0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir\n",
    "log_dir = run_name+\"/TRPO/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,\n",
    "                             deterministic=True, render=False,verbose=0)\n",
    "\n",
    "# Initialize the RL agent\n",
    "model = TRPO(\"MlpPolicy\", env,tensorboard_log=tsb_dir)\n",
    "model.learn(total_timesteps=1_500_000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300d2423-68ff-4df4-8c47-66ce0b6856ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "total_ev_served:  44.98\n",
      "total_profits:  700.0134845769985\n",
      "real_profits (no flexibility):  -162.54774679992187\n",
      "Up_flexibility (kWh):  3388.626793412494\n",
      "Down_flexibility (kWh):  727.3494636618823\n",
      "total_energy_charged:  788.8162455983061\n",
      "average_user_satisfaction:  0.852549868528215\n",
      "energy_user_satisfaction:  100.0\n",
      "reward:  432.59859450999994\n"
     ]
    }
   ],
   "source": [
    "from ev2gym.utilities.evaluators import evaluate_model\n",
    "\n",
    "# Load the best model and put the enviroment\n",
    "model = TRPO.load(log_dir+\"best_model.zip\")\n",
    "model.set_env(env)\n",
    "\n",
    "# Custom model evaluator\n",
    "evaluate_model(model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02a061-abc3-476a-b6cc-97faae87a8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EVthesis",
   "language": "python",
   "name": "evth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
